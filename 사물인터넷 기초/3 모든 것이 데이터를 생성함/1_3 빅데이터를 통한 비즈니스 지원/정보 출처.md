정보 출처
대규모 데이터 세트를 구성하는 데이터의 소스는 다양하다. 센서 데이터 외에도 아래와 같은 소스로부터 스캔되고 입력되고 인터넷으로 릴리스 되는 여러 데이터가 존재한다.

소셜 미디어 사이트 - 페이스북, 유튜브, 이하모니, 트위터
인터넷상의 HTTP, 웹 페이지, 검색 엔진
공용 및 개인 아카이브의 데이터 기록
이메일, 문서 전송, 사진에 붙는 메타데이터
의료 양식, 보험 양식, 세금 양식
DNA를 이용한 유전체학 연구
수집된 데이터는 구조화 또는 비구조화 데이터로 분류할 수 있다.

구조화 데이터는 스프레드시트 또는 의료 양식과 같은 “고정된” 형식의 입력을 사용하는 응용 프로그램에 의해 생성된다. 데이터가 구조화된 것으로 간주되더라도 응용 프로그램마다 서로 다른 형식의 파일이 만들어지며 서로 호환되지 않을 수도 있다. 구조화 데이터는 CSV와 같은 일반 형식으로 조작될 필요가 있을 수도 있다.

CSV(쉼표로 구분된 값) 파일은 쉼표를 사용하여 데이터 테이블의 열을 구분하고 캐리지 리턴 문자를 별도의 행으로 사용하는 평문 파일의 한 유형이다. 각 행은 레코드이다. 전형적인 데이터베이스와 스프레드시트에서 가져오기와 내보내기를 위한 일반적인 방법으로 사용되지만 특별한 표준은 없다. JSON과 XML은 표준에 따라 데이터 레코드를 표현하는 평문 파일 유형들이다. 이러한 파일 형식은 다양한 응용 프로그램과 호환된다. 데이터를 공통 형식으로 변환하는 것은 서로 다른 소스의 데이터를 결합하는 중요한 방법이다.

비구조화 데이터는 오디오, 비디오, 웹 페이지, 트윗과 같이 “자유형” 스타일로 생성된다. 비구조화 데이터를 처리하고 분석하기 위해서는 다른 도구들이 필요하다. 다음은 두 가지 예이다.

웹 페이지는 기계가 아닌 인간에게 데이터를 제공하기 위해 만들어진다. “웹 스크래핑” 도구는 HTML 페이지에서 데이터를 자동으로 추출한다. 이는 검색 엔진의 스파이더나 웹 크롤러와 유사하다. 웹을 탐색하여 데이터를 추출하고 검색 쿼리에 응답하기 위한 데이터베이스를 만든다. 웹 스크래핑 소프트웨어는 하이퍼 텍스트 전송 프로토콜이나 웹 브라우저를 사용하여 월드 와이드 웹에 액세스 할 수 있다. 일반적으로 웹 스크래핑은 봇이나 웹 크롤러를 사용하여 데이터 마이닝을 수행하는 자동화된 프로세스이다. 특정 데이터가 수집되어 웹에서 데이터베이스나 스프레드시트로 복사된다. 이러한 과정을 통해 데이터를 쉽게 분석할 수 있다.
페이스 북과 같은 많은 대형 웹 서비스 제공 업체는 응용 프로그래밍 인터페이스(API) 를 사용하여 자동으로 데이터를 수집하는 표준화 된 인터페이스를 제공한다. 가장 일반적인 방법은 RESTful API를 사용하는 것이다. RESTful API는 통신 프로토콜로 HTTP를, 데이터 인코딩을 위해 JSON 구조를 사용한다. 구글과 트위터와 같은 인터넷 웹 사이트는 많은 양의 정적 및 시계열 데이터를 수집한다. 이러한 사이트의 API에 대하여 잘 이해함으로써 데이터 분석가와 엔지니어는 인터넷에서 지속적으로 생성되는 대량의 데이터에 액세스할 수 있다.