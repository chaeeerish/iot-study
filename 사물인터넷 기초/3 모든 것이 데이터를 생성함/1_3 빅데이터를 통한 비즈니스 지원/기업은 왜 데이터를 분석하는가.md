분산 처리
데이터 관리 관점에서 볼 때, 인간이 생성한 데이터를 분석하는 것은 단순했다. 데이터의 양은 관리가 쉬웠고 살펴보기도 상대적으로 쉬웠다. 그러나 비즈니스 자동화 시스템이 폭발적으로 증가하고 웹 애플리케이션과 기계가 생성한 데이터가 기하급수적으로 증가함에 따라, 데이터 분석 관리가 점점 더 어려워지게 되었다. 실제로 현재 존재하는 데이터의 90%는 지난 2년 동안 생성된 것이다. 단기간에 이렇게 데이터가 급증한 것은 기하 급수적 성장의 특징으로 볼 수 있다. 이렇게 많은 양의 데이터를 적절한 시간 안에 처리하고 분석하기는 어렵다.

대규모 데이터베이스를 크고 강력한 메인프레임 컴퓨터에서 처리하고 거대한 디스크 어레이에 저장하는 대신(수직 확장), 분산 데이터 처리를 통해 대량의 데이터를 여러 개의 작은 조각으로 나눈다. 이러한 작은 데이터 볼륨은 여러 위치에 분산되어 더 작은 프로세서를 가진 많은 컴퓨터에서 처리된다. 분산 아키텍처에서 각각의 컴퓨터는 빅 데이터 전체 중 일부만을 분석한다(수평 확장).

대부분의 분산 파일 시스템은 클라이언트 프로그램에서 보이지 않도록 설계되었다. 분산 파일 시스템은 파일을 검색하고 이동할 수 있지만, 사용자는 파일이 여러 다른 서버나 노드에 분산되어 있다는 것을 알 수 없다. 사용자는 이들 파일이 자신의 로컬 컴퓨터에 있는 것처럼 사용할 수 있다. 모든 사용자가 파일 시스템을 동일하게 볼 수 있으며 다른 사용자와 동시에 데이터에 접근할 수 있다.

Hadoop은 이러한 빅 데이터 볼륨을 처리하기 위해 만들어졌다. Hadoop 프로젝트는 두 가지 측면으로 시작했다. HDFS(Hadoop Distributed File System)는 분산형 고장감내형 파일 시스템이고, MapReduce는 데이터를 분산 방식으로 처리하는 것이다. Hadoop은 이제 빅 데이터 관리를 위한 매우 포괄적인 소프트웨어 에코시스템으로 발전했다.

Hadoop은 오픈 소스 소프트웨어로서, 컴퓨터 클러스터에 저장되는 테라바이트 크기의 대용량 데이터 세트에 대한 분산 처리를 가능하게 한다. Hadoop은 단일 서버에서 수천 대의 머신으로 확장하도록 설계되었으며 각 머신은 로컬 연산 및 스토리지를 제공한다. 보다 효율적으로 만들기 위해, Hadoop을 설치하고 많은 VM에서 실행할 수 있다. 이러한 VM은 모두 병렬로 함께 작동하여 데이터를 처리하고 저장할 수 있다.

Hadoop이 빅 데이터 처리를 위한 업계 표준으로 되게 된 주요한 두 가지 특징이 있다.

확장성 – 클러스터 크기가 클수록 성능이 향상되고 데이터 처리 기능이 향상된다. Hadoop을 사용하면 관리 부담을 과도하게 늘리지 않고도 클러스터 크기를 5개정도의 노드로 된 클러스터에서 천개 노드를 가지는 클러스터에 이르기까지 쉽게 확장할 수 있다.
고장감내성 – Hadoop은 클러스터 간에 데이터를 자동으로 복제하여 데이터가 손실되지 않도록 한다. 디스크, 노드, 또는 전체 랙에 장애가 발생해도 데이터는 안전하게 유지된다.